{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yno3fm36xqnnc8/logistic-regression-the-jax-way?scriptVersionId=139524696\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"aa69c684","metadata":{"papermill":{"duration":0.008891,"end_time":"2023-08-10T13:46:25.582958","exception":false,"start_time":"2023-08-10T13:46:25.574067","status":"completed"},"tags":[]},"source":["## Logistic Regression the JAX Way"]},{"cell_type":"code","execution_count":1,"id":"b1cb29ba","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:25.601797Z","iopub.status.busy":"2023-08-10T13:46:25.601416Z","iopub.status.idle":"2023-08-10T13:46:27.092715Z","shell.execute_reply":"2023-08-10T13:46:27.091339Z"},"papermill":{"duration":1.504205,"end_time":"2023-08-10T13:46:27.095713","exception":false,"start_time":"2023-08-10T13:46:25.591508","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd\n","import jax\n","import jax.numpy as jnp\n","from collections import namedtuple"]},{"cell_type":"markdown","id":"80cc77d7","metadata":{"papermill":{"duration":0.008502,"end_time":"2023-08-10T13:46:27.11297","exception":false,"start_time":"2023-08-10T13:46:27.104468","status":"completed"},"tags":[]},"source":["The goal of this notebook is to demonstrate how to do logistic regression with the JAX library. I'm sure there are other, better ways to do this, but this is a good start. For this example, let's use the Titanic dataset found on Kaggle."]},{"cell_type":"code","execution_count":2,"id":"dd6e9866","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:27.135311Z","iopub.status.busy":"2023-08-10T13:46:27.134621Z","iopub.status.idle":"2023-08-10T13:46:27.686055Z","shell.execute_reply":"2023-08-10T13:46:27.684796Z"},"papermill":{"duration":0.564477,"end_time":"2023-08-10T13:46:27.688817","exception":false,"start_time":"2023-08-10T13:46:27.12434","status":"completed"},"tags":[]},"outputs":[],"source":["test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n","train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n","\n","pclasses = jnp.array(train_data['Pclass']).reshape((-1, 1))\n","survived = jnp.array(train_data['Survived']).reshape((-1, 1))"]},{"cell_type":"markdown","id":"7815e836","metadata":{"papermill":{"duration":0.008111,"end_time":"2023-08-10T13:46:27.707448","exception":false,"start_time":"2023-08-10T13:46:27.699337","status":"completed"},"tags":[]},"source":["Our logistic regression model requires two parameters, the weight $w$ and the bias $b$. We will regress over a single feature: passenger class number."]},{"cell_type":"code","execution_count":3,"id":"45e44bce","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:27.727132Z","iopub.status.busy":"2023-08-10T13:46:27.726678Z","iopub.status.idle":"2023-08-10T13:46:27.732207Z","shell.execute_reply":"2023-08-10T13:46:27.731378Z"},"papermill":{"duration":0.018214,"end_time":"2023-08-10T13:46:27.734813","exception":false,"start_time":"2023-08-10T13:46:27.716599","status":"completed"},"tags":[]},"outputs":[],"source":["LogisticRegressionParams = namedtuple('LogisticRegressionParams', 'w b')"]},{"cell_type":"markdown","id":"1de75e95","metadata":{"papermill":{"duration":0.008173,"end_time":"2023-08-10T13:46:27.751827","exception":false,"start_time":"2023-08-10T13:46:27.743654","status":"completed"},"tags":[]},"source":["First we define the `predict` function. It takes the model parameters `params` and some regressors `x`, and uses them to create a single prediction. This particular model computes $f(x\\cdot{w} + b)$, where $f(z) = \\frac{1}{1+e^{-z}}$ is a logistic function."]},{"cell_type":"code","execution_count":4,"id":"af41b16a","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:27.77189Z","iopub.status.busy":"2023-08-10T13:46:27.771109Z","iopub.status.idle":"2023-08-10T13:46:27.777689Z","shell.execute_reply":"2023-08-10T13:46:27.776487Z"},"papermill":{"duration":0.019735,"end_time":"2023-08-10T13:46:27.780226","exception":false,"start_time":"2023-08-10T13:46:27.760491","status":"completed"},"tags":[]},"outputs":[],"source":["@jax.jit\n","def sigmoid_predict(params: LogisticRegressionParams, x: jnp.array) -> jnp.array:\n","    z = x.dot(params.w) + params.b\n","    return jax.nn.sigmoid(z)"]},{"cell_type":"markdown","id":"02d68885","metadata":{"papermill":{"duration":0.008132,"end_time":"2023-08-10T13:46:27.796852","exception":false,"start_time":"2023-08-10T13:46:27.78872","status":"completed"},"tags":[]},"source":["Another model, presented below, uses the `softmax` function to determine a probability distribution over the two possible states, $0$ (dead) and $1$ (living). Specifically, the value $${z} = {w}{x} + {b}$$ is computed. In our case, this leaves us with a two-dimensional vector which is fed to the softmax function, mapping $(u,v)$ to $(\\frac{\\exp{u}}{\\exp{u} +\\exp{v}}, \\frac{\\exp{v}}{\\exp{u} +\\exp{v}})$.\n","\n","Observe the `@jax.jit` decorator. This tells JAX to just-in-time compile our prediction function. Not all functions can be jitted, see [this](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#why-can-t-we-just-jit-everything) for more."]},{"cell_type":"code","execution_count":5,"id":"f64d49f3","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:27.816733Z","iopub.status.busy":"2023-08-10T13:46:27.815943Z","iopub.status.idle":"2023-08-10T13:46:27.822363Z","shell.execute_reply":"2023-08-10T13:46:27.821445Z"},"papermill":{"duration":0.01896,"end_time":"2023-08-10T13:46:27.824849","exception":false,"start_time":"2023-08-10T13:46:27.805889","status":"completed"},"tags":[]},"outputs":[],"source":["@jax.jit\n","def softmax_predict(params: LogisticRegressionParams, x: jnp.array) -> jnp.array:\n","    z = params.w.transpose() @ x + params.b\n","    return jax.nn.softmax(z)"]},{"cell_type":"markdown","id":"2450da7c","metadata":{"papermill":{"duration":0.008258,"end_time":"2023-08-10T13:46:27.841708","exception":false,"start_time":"2023-08-10T13:46:27.83345","status":"completed"},"tags":[]},"source":["The `vpredict` function shows how we can vectorize functions with JAX, allowing us to compute predictions in batches."]},{"cell_type":"code","execution_count":6,"id":"41501dbb","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:27.862454Z","iopub.status.busy":"2023-08-10T13:46:27.86199Z","iopub.status.idle":"2023-08-10T13:46:27.870433Z","shell.execute_reply":"2023-08-10T13:46:27.869014Z"},"papermill":{"duration":0.021109,"end_time":"2023-08-10T13:46:27.872915","exception":false,"start_time":"2023-08-10T13:46:27.851806","status":"completed"},"tags":[]},"outputs":[],"source":["@jax.jit\n","def sigmoid_vpredict(params: LogisticRegressionParams, regressors: jnp.array) -> jnp.array:\n","    f = jax.vmap(sigmoid_predict, in_axes=(None, 0))\n","    return f(params, regressors)\n","@jax.jit\n","def softmax_vpredict(params: LogisticRegressionParams, regressors: jnp.array) -> jnp.array:\n","    f = jax.vmap(softmax_predict, in_axes=(None, 0))\n","    return f(params, regressors)"]},{"cell_type":"markdown","id":"0d9320fc","metadata":{"papermill":{"duration":0.008335,"end_time":"2023-08-10T13:46:27.890306","exception":false,"start_time":"2023-08-10T13:46:27.881971","status":"completed"},"tags":[]},"source":["When we train the sigmoid model, we'll use the cross-entropy loss function. This can be thought of as an analogue of \"distance\" for probability distributions, so our goal is to make the prediction distribution (our model's output) as close as possible to the empirical distribution."]},{"cell_type":"code","execution_count":7,"id":"7f573f23","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:27.91063Z","iopub.status.busy":"2023-08-10T13:46:27.910205Z","iopub.status.idle":"2023-08-10T13:46:27.916736Z","shell.execute_reply":"2023-08-10T13:46:27.915804Z"},"papermill":{"duration":0.019716,"end_time":"2023-08-10T13:46:27.919097","exception":false,"start_time":"2023-08-10T13:46:27.899381","status":"completed"},"tags":[]},"outputs":[],"source":["@jax.jit\n","def crossent(params: jnp.array, features: jnp.array, labels: jnp.array) -> jnp.array:\n","    predictions = sigmoid_vpredict(params, features)\n","    a = labels * jnp.log(predictions)\n","    b = (1.0 - labels) * jnp.log(1.0 - predictions)\n","    return -jnp.mean(a + b)"]},{"cell_type":"markdown","id":"824c1ea4","metadata":{"papermill":{"duration":0.008411,"end_time":"2023-08-10T13:46:27.936271","exception":false,"start_time":"2023-08-10T13:46:27.92786","status":"completed"},"tags":[]},"source":["It's time to train the logistic model. This is accomplished by gradient descent. The gradient of the cross-entropy loss function is determined using `jax.grad`. The gradient over the entire training set is computed and the model parameters are updated according to the rule\n","$$\n","p \\leftarrow p - \\eta \\nabla{\\ell},\n","$$\n","where $\\eta$ is the learning rate (set here to $0.01$) and $\\ell$ is the loss function."]},{"cell_type":"code","execution_count":8,"id":"8c044b9a","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:27.95714Z","iopub.status.busy":"2023-08-10T13:46:27.956483Z","iopub.status.idle":"2023-08-10T13:46:31.529699Z","shell.execute_reply":"2023-08-10T13:46:31.528426Z"},"papermill":{"duration":3.587351,"end_time":"2023-08-10T13:46:31.532477","exception":false,"start_time":"2023-08-10T13:46:27.945126","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["2.227012\n","0.7300361\n","0.625376\n","0.6208769\n","0.620122\n","0.6195703\n","0.61905324\n","0.61856127\n","0.6180927\n","0.61764634\n"]}],"source":["learning_rate = 1e-2\n","sigmoid_params = LogisticRegressionParams(jnp.array(1.0), jnp.array(1.0))\n","crossent_grad_fn = jax.grad(crossent)\n","\n","for i in range(1_000):\n","    if i % 100 == 0:\n","        print(crossent(sigmoid_params, pclasses, survived))\n","    grads = crossent_grad_fn(sigmoid_params, pclasses, survived)\n","    sigmoid_params = LogisticRegressionParams(sigmoid_params.w - learning_rate * grads.w, sigmoid_params.b - learning_rate * grads.b)\n"]},{"cell_type":"markdown","id":"3aa1b7ca","metadata":{"papermill":{"duration":0.009054,"end_time":"2023-08-10T13:46:31.551162","exception":false,"start_time":"2023-08-10T13:46:31.542108","status":"completed"},"tags":[]},"source":["Now that the model is trained, let's see how well it performs."]},{"cell_type":"code","execution_count":9,"id":"46178372","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:31.572385Z","iopub.status.busy":"2023-08-10T13:46:31.571943Z","iopub.status.idle":"2023-08-10T13:46:31.749776Z","shell.execute_reply":"2023-08-10T13:46:31.748501Z"},"papermill":{"duration":0.192016,"end_time":"2023-08-10T13:46:31.752477","exception":false,"start_time":"2023-08-10T13:46:31.560461","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy is 67.90%\n"]}],"source":["preds = round(sigmoid_vpredict(sigmoid_params, pclasses))\n","accuracy = 1.0 - abs(preds-survived).mean()\n","print(f\"accuracy is {100.0*accuracy:.2f}%\")"]},{"cell_type":"markdown","id":"de3dd8d5","metadata":{"papermill":{"duration":0.009251,"end_time":"2023-08-10T13:46:31.771206","exception":false,"start_time":"2023-08-10T13:46:31.761955","status":"completed"},"tags":[]},"source":["In this section, the softmax model is trained analogously to the sigmoid model."]},{"cell_type":"markdown","id":"ff1e2b57","metadata":{"papermill":{"duration":0.008938,"end_time":"2023-08-10T13:46:31.789384","exception":false,"start_time":"2023-08-10T13:46:31.780446","status":"completed"},"tags":[]},"source":["When we train the softmax model, we'll use the the `nll` function, which computes the negative log-likelihood of the data as a function of `params`. That is, it computes the probability of the observed data given the model. The `take_along_axis` function is used to index the predictions, retrieving the probability of the particular occurence. Finally the mean is taken across the whole batch."]},{"cell_type":"code","execution_count":10,"id":"0b7844f7","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:31.809874Z","iopub.status.busy":"2023-08-10T13:46:31.809453Z","iopub.status.idle":"2023-08-10T13:46:31.816876Z","shell.execute_reply":"2023-08-10T13:46:31.81539Z"},"papermill":{"duration":0.020439,"end_time":"2023-08-10T13:46:31.819139","exception":false,"start_time":"2023-08-10T13:46:31.7987","status":"completed"},"tags":[]},"outputs":[],"source":["@jax.jit\n","def nll(params: LogisticRegressionParams, regressors: jnp.array, labels: jnp.array):\n","    probs = softmax_vpredict(params, regressors)\n","    log_probs = jnp.log(probs)\n","    return -jnp.take_along_axis(log_probs, labels, 1).mean()"]},{"cell_type":"code","execution_count":11,"id":"6e8431d6","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:31.840649Z","iopub.status.busy":"2023-08-10T13:46:31.839367Z","iopub.status.idle":"2023-08-10T13:46:35.46695Z","shell.execute_reply":"2023-08-10T13:46:35.465653Z"},"papermill":{"duration":3.641172,"end_time":"2023-08-10T13:46:35.469736","exception":false,"start_time":"2023-08-10T13:46:31.828564","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6931472\n","0.6381312\n","0.6353234\n","0.63285196\n","0.6306094\n","0.6285747\n","0.62672895\n","0.6250547\n","0.6235363\n","0.62215906\n"]}],"source":["learning_rate = 1e-2\n","softmax_params = LogisticRegressionParams(jnp.zeros([1, 2]), jnp.zeros([2]))\n","nll_grad_fn = jax.grad(nll)\n","for i in range(1_000):\n","    if i % 100 == 0:\n","        print(nll(softmax_params, pclasses, survived))\n","    grads = nll_grad_fn(softmax_params, pclasses, survived)\n","    softmax_params = LogisticRegressionParams(softmax_params.w - learning_rate * grads[0], softmax_params.b - learning_rate * grads[1])"]},{"cell_type":"markdown","id":"678cbab6","metadata":{"papermill":{"duration":0.009974,"end_time":"2023-08-10T13:46:35.490226","exception":false,"start_time":"2023-08-10T13:46:35.480252","status":"completed"},"tags":[]},"source":["Now let's check the softmax model's accuracy."]},{"cell_type":"code","execution_count":12,"id":"14be2b4b","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:35.514648Z","iopub.status.busy":"2023-08-10T13:46:35.514244Z","iopub.status.idle":"2023-08-10T13:46:35.617098Z","shell.execute_reply":"2023-08-10T13:46:35.615714Z"},"papermill":{"duration":0.118502,"end_time":"2023-08-10T13:46:35.61999","exception":false,"start_time":"2023-08-10T13:46:35.501488","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy is 67.90%\n"]}],"source":["preds = softmax_vpredict(softmax_params, pclasses).argmax(axis=1).reshape((-1, 1))\n","accuracy = 1.0 - abs(preds-survived).mean()\n","print(f\"accuracy is {100.0*accuracy:.2f}%\")"]},{"cell_type":"markdown","id":"1af9fef6","metadata":{"papermill":{"duration":0.010241,"end_time":"2023-08-10T13:46:35.641059","exception":false,"start_time":"2023-08-10T13:46:35.630818","status":"completed"},"tags":[]},"source":["The predictions made by the two models are compared."]},{"cell_type":"code","execution_count":13,"id":"4a820944","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:35.664815Z","iopub.status.busy":"2023-08-10T13:46:35.664423Z","iopub.status.idle":"2023-08-10T13:46:35.890709Z","shell.execute_reply":"2023-08-10T13:46:35.889684Z"},"papermill":{"duration":0.240965,"end_time":"2023-08-10T13:46:35.893113","exception":false,"start_time":"2023-08-10T13:46:35.652148","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","      <th>Survived</th>\n","      <th>SurvivedSoftmax</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>892</td>\n","      <td>3</td>\n","      <td>Kelly, Mr. James</td>\n","      <td>male</td>\n","      <td>34.5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>330911</td>\n","      <td>7.8292</td>\n","      <td>NaN</td>\n","      <td>Q</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>893</td>\n","      <td>3</td>\n","      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n","      <td>female</td>\n","      <td>47.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>363272</td>\n","      <td>7.0000</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>894</td>\n","      <td>2</td>\n","      <td>Myles, Mr. Thomas Francis</td>\n","      <td>male</td>\n","      <td>62.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>240276</td>\n","      <td>9.6875</td>\n","      <td>NaN</td>\n","      <td>Q</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>895</td>\n","      <td>3</td>\n","      <td>Wirz, Mr. Albert</td>\n","      <td>male</td>\n","      <td>27.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>315154</td>\n","      <td>8.6625</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>896</td>\n","      <td>3</td>\n","      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n","      <td>female</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3101298</td>\n","      <td>12.2875</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>413</th>\n","      <td>1305</td>\n","      <td>3</td>\n","      <td>Spector, Mr. Woolf</td>\n","      <td>male</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>A.5. 3236</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>414</th>\n","      <td>1306</td>\n","      <td>1</td>\n","      <td>Oliva y Ocana, Dona. Fermina</td>\n","      <td>female</td>\n","      <td>39.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>PC 17758</td>\n","      <td>108.9000</td>\n","      <td>C105</td>\n","      <td>C</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>415</th>\n","      <td>1307</td>\n","      <td>3</td>\n","      <td>Saether, Mr. Simon Sivertsen</td>\n","      <td>male</td>\n","      <td>38.5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>SOTON/O.Q. 3101262</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>416</th>\n","      <td>1308</td>\n","      <td>3</td>\n","      <td>Ware, Mr. Frederick</td>\n","      <td>male</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>359309</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>417</th>\n","      <td>1309</td>\n","      <td>3</td>\n","      <td>Peter, Master. Michael J</td>\n","      <td>male</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2668</td>\n","      <td>22.3583</td>\n","      <td>NaN</td>\n","      <td>C</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>418 rows × 13 columns</p>\n","</div>"],"text/plain":["     PassengerId  Pclass                                          Name  \\\n","0            892       3                              Kelly, Mr. James   \n","1            893       3              Wilkes, Mrs. James (Ellen Needs)   \n","2            894       2                     Myles, Mr. Thomas Francis   \n","3            895       3                              Wirz, Mr. Albert   \n","4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n","..           ...     ...                                           ...   \n","413         1305       3                            Spector, Mr. Woolf   \n","414         1306       1                  Oliva y Ocana, Dona. Fermina   \n","415         1307       3                  Saether, Mr. Simon Sivertsen   \n","416         1308       3                           Ware, Mr. Frederick   \n","417         1309       3                      Peter, Master. Michael J   \n","\n","        Sex   Age  SibSp  Parch              Ticket      Fare Cabin Embarked  \\\n","0      male  34.5      0      0              330911    7.8292   NaN        Q   \n","1    female  47.0      1      0              363272    7.0000   NaN        S   \n","2      male  62.0      0      0              240276    9.6875   NaN        Q   \n","3      male  27.0      0      0              315154    8.6625   NaN        S   \n","4    female  22.0      1      1             3101298   12.2875   NaN        S   \n","..      ...   ...    ...    ...                 ...       ...   ...      ...   \n","413    male   NaN      0      0           A.5. 3236    8.0500   NaN        S   \n","414  female  39.0      0      0            PC 17758  108.9000  C105        C   \n","415    male  38.5      0      0  SOTON/O.Q. 3101262    7.2500   NaN        S   \n","416    male   NaN      0      0              359309    8.0500   NaN        S   \n","417    male   NaN      1      1                2668   22.3583   NaN        C   \n","\n","     Survived  SurvivedSoftmax  \n","0           0                0  \n","1           0                0  \n","2           0                0  \n","3           0                0  \n","4           0                0  \n","..        ...              ...  \n","413         0                0  \n","414         1                1  \n","415         0                0  \n","416         0                0  \n","417         0                0  \n","\n","[418 rows x 13 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["pclasses = jnp.array(test_data['Pclass']).reshape((-1, 1))\n","\n","test_data['Survived'] = round(sigmoid_vpredict(sigmoid_params, pclasses))\n","test_data['Survived'] = test_data['Survived'].apply(lambda x: int(x))\n","\n","test_data['SurvivedSoftmax'] = softmax_vpredict(softmax_params, pclasses).argmax(axis=1)\n","test_data['SurvivedSoftmax'] = test_data['SurvivedSoftmax'].apply(lambda x: int(x))\n","\n","test_data"]},{"cell_type":"code","execution_count":14,"id":"75ce85aa","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:35.917024Z","iopub.status.busy":"2023-08-10T13:46:35.916442Z","iopub.status.idle":"2023-08-10T13:46:35.924655Z","shell.execute_reply":"2023-08-10T13:46:35.923421Z"},"papermill":{"duration":0.023105,"end_time":"2023-08-10T13:46:35.927283","exception":false,"start_time":"2023-08-10T13:46:35.904178","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["True"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["(test_data['Survived'] == test_data['SurvivedSoftmax']).all()"]},{"cell_type":"markdown","id":"a0ad50a8","metadata":{"papermill":{"duration":0.011101,"end_time":"2023-08-10T13:46:35.949544","exception":false,"start_time":"2023-08-10T13:46:35.938443","status":"completed"},"tags":[]},"source":["This shows that the predictions are identical across models.\n","\n","Finally, generate predictions and save them to `/kaggle/working/submission.csv`."]},{"cell_type":"code","execution_count":15,"id":"9c573051","metadata":{"execution":{"iopub.execute_input":"2023-08-10T13:46:35.97387Z","iopub.status.busy":"2023-08-10T13:46:35.973452Z","iopub.status.idle":"2023-08-10T13:46:35.986138Z","shell.execute_reply":"2023-08-10T13:46:35.985296Z"},"papermill":{"duration":0.027821,"end_time":"2023-08-10T13:46:35.988579","exception":false,"start_time":"2023-08-10T13:46:35.960758","status":"completed"},"tags":[]},"outputs":[],"source":["test_data[['PassengerId', 'Survived']].set_index(\"PassengerId\").to_csv(\"/kaggle/working/submission.csv\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":23.54355,"end_time":"2023-08-10T13:46:37.023225","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-08-10T13:46:13.479675","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}