{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yno3fm36xqnnc8/logistic-regression-the-jax-way?scriptVersionId=139166041\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Logistic Regression the JAX Way","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport jax\nimport jax.numpy as jnp\nfrom collections import namedtuple","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:35:18.089425Z","iopub.execute_input":"2023-08-07T09:35:18.089813Z","iopub.status.idle":"2023-08-07T09:35:18.095094Z","shell.execute_reply.started":"2023-08-07T09:35:18.089783Z","shell.execute_reply":"2023-08-07T09:35:18.094139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The goal of this notebook is to demonstrate how to do logistic regression with the JAX library. I'm sure there are other, better ways to do this, but this is a good start. For this example, let's use the Titanic dataset found on Kaggle. ","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n\npclasses = train_data['Pclass']\npclasses = jnp.array(pclasses).reshape((-1, 1))\n\nsurvived = train_data['Survived']\nsurvived = jnp.array(survived).reshape((-1, 1))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:35:18.097122Z","iopub.execute_input":"2023-08-07T09:35:18.09752Z","iopub.status.idle":"2023-08-07T09:35:18.122743Z","shell.execute_reply.started":"2023-08-07T09:35:18.097486Z","shell.execute_reply":"2023-08-07T09:35:18.121772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our logistic regression model requires two parameters, the weight $w$ and the bias $b$. We will regress over a single feature: passenger class number.","metadata":{}},{"cell_type":"code","source":"LogisticRegressionParams = namedtuple('LogisticRegressionParams', 'w b')\nmodel_params = LogisticRegressionParams(jnp.zeros([1, 2]), jnp.zeros([2]))","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:35:18.124192Z","iopub.execute_input":"2023-08-07T09:35:18.124803Z","iopub.status.idle":"2023-08-07T09:35:18.130458Z","shell.execute_reply.started":"2023-08-07T09:35:18.124771Z","shell.execute_reply":"2023-08-07T09:35:18.129549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we define the `predict` function. It takes the model parameters `params` and some regressors `x`, and uses them to create a single prediction. The model implemented here uses the `softmax` function to determine a probability distribution over the two possible states, $0$ (dead) and $1$ (living). Specifically, the value $${z} = {w}{x} + {b}$$ is computed. In our case, this leaves us with a two-dimensional vector which is fed to the softmax function, mapping $(u,v)$ to $(\\frac{\\exp{u}}{\\exp{u} +\\exp{v}}, \\frac{\\exp{v}}{\\exp{u} +\\exp{v}})$.\n\nObserve the `@jax.jit` decorator. This tells JAX to just-in-time compile our prediction function. Not all functions can be jitted, see [this](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#why-can-t-we-just-jit-everything) for more.","metadata":{}},{"cell_type":"code","source":"@jax.jit\ndef predict(params: LogisticRegressionParams, x: jnp.array):\n    z = params.w.transpose() @ x + params.b\n    return jax.nn.softmax(z)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:35:18.132777Z","iopub.execute_input":"2023-08-07T09:35:18.133194Z","iopub.status.idle":"2023-08-07T09:35:18.146071Z","shell.execute_reply.started":"2023-08-07T09:35:18.133161Z","shell.execute_reply":"2023-08-07T09:35:18.144907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `vpredict` function shows how we can vectorize functions with JAX, allowing us to compute predictions in batches.","metadata":{}},{"cell_type":"code","source":"@jax.jit\ndef vpredict(params, regressors):\n    f = jax.vmap(lambda x: predict(params, x))\n    return f(regressors)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:35:18.147817Z","iopub.execute_input":"2023-08-07T09:35:18.148218Z","iopub.status.idle":"2023-08-07T09:35:18.158772Z","shell.execute_reply.started":"2023-08-07T09:35:18.148186Z","shell.execute_reply":"2023-08-07T09:35:18.157862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `nll` function computes the negative log-likelihood of the data as a function of `params`. That is, it computes the probability of the observed data given the model. The `take_along_axis` function is used to index the predictions, retrieving the probability of the particular occurence. Finally the mean is taken across the whole batch.","metadata":{}},{"cell_type":"code","source":"@jax.jit\ndef nll(params: LogisticRegressionParams, regressors: jnp.array, labels: jnp.array):\n    probs = vpredict(params, regressors)\n    log_probs = jnp.log(probs)\n    return -jnp.take_along_axis(log_probs, labels, 1).mean()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:35:18.160045Z","iopub.execute_input":"2023-08-07T09:35:18.160996Z","iopub.status.idle":"2023-08-07T09:35:18.174521Z","shell.execute_reply.started":"2023-08-07T09:35:18.160963Z","shell.execute_reply":"2023-08-07T09:35:18.173511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to train the model. This is accomplished by gradient descent. The gradient of the negative log-likelihood function `nll` is determined using `jax.grad`. The gradient over the entire training set is computed and the model parameters are updated according to the rule\n$$\np \\leftarrow p - \\eta \\nabla{\\ell},\n$$\nwhere $\\eta$ is the learning rate (set here to $0.01$).","metadata":{}},{"cell_type":"code","source":"learning_rate = 1e-2\nloss_grad_fn = jax.grad(nll)\nfor i in range(1_000):\n    if i % 100 == 0:\n        print(nll(model_params, pclasses, survived))\n    grads = loss_grad_fn(model_params, pclasses, survived)\n    model_params = LogisticRegressionParams(model_params.w - learning_rate * grads[0], model_params.b - learning_rate * grads[1])","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:35:18.175801Z","iopub.execute_input":"2023-08-07T09:35:18.177096Z","iopub.status.idle":"2023-08-07T09:35:21.907396Z","shell.execute_reply.started":"2023-08-07T09:35:18.177027Z","shell.execute_reply":"2023-08-07T09:35:21.906132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the model is trained, let's see how well it performs.","metadata":{}},{"cell_type":"code","source":"f = jax.vmap(lambda x: predict(model_params, x))\npreds = f(pclasses).argmax(axis=1).reshape((-1, 1))\naccuracy = 1.0 - abs(preds-survived).mean()\nprint(f\"accuracy is {100.0*accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:35:21.910078Z","iopub.execute_input":"2023-08-07T09:35:21.910809Z","iopub.status.idle":"2023-08-07T09:35:22.118541Z","shell.execute_reply.started":"2023-08-07T09:35:21.910755Z","shell.execute_reply":"2023-08-07T09:35:22.117189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we generate predictions and save it to `/kaggle/working/submission.csv`.","metadata":{}},{"cell_type":"code","source":"pclasses = test_data['Pclass']\npclasses = jnp.array(pclasses).reshape((-1, 1))\nf = jax.vmap(lambda x: predict(model_params, x))\ntest_preds = f(pclasses).argmax(axis=1).reshape((-1, 1))\ntest_data['Survived'] = test_preds\ntest_data['Survived'] = test_data['Survived'].apply(lambda x: int(x))\ntest_data[['PassengerId', 'Survived']].set_index(\"PassengerId\").to_csv(\"/kaggle/working/submission.csv\")\ntest_data","metadata":{"execution":{"iopub.status.busy":"2023-08-07T09:44:58.280396Z","iopub.execute_input":"2023-08-07T09:44:58.28087Z","iopub.status.idle":"2023-08-07T09:44:58.320582Z","shell.execute_reply.started":"2023-08-07T09:44:58.280835Z","shell.execute_reply":"2023-08-07T09:44:58.319189Z"},"trusted":true},"execution_count":null,"outputs":[]}]}