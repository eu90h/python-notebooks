{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive models form a simple but useful class of timeseries models. Consider a timeseries $\\{x_t\\}_{t=1}^N$ of $N$ observations of the Billboard ranking of a particular song. Perhaps the simplest possible model for predicting the next observed ranking would be as an affine function of the current ranking, i.e.\n",
    "$$\n",
    "\\hat{x}_{t+1} = wx_t + b\n",
    "$$.\n",
    "This is an first-order autoregressive model, written AR(1). In general we may have $m$ variables so that\n",
    "$$\n",
    "\\hat{x}_{t+1} = \\sum^m_{i=1}{w_i x_{t-i}} + b\n",
    "$$, \n",
    "giving us an $m$-th order autoregressive model, or AR(m).\n",
    "\n",
    "These models use no other information other than the series itself. They serve as a useful baseline with which to compare other more complex models. For instance, Google search volume has been shown to have utility for predicting a number of things, such as unemployment levels, stock prices, auto and home sales, and even disease prevalence. One would assume that search volume would be useful for predicting Billboard rankings, and indeed it is. However, as Goel, et al. show in their paper [Predicting consumer behavior with Web search](https://www.pnas.org/doi/10.1073/pnas.1005962107), this model is outperformed by a simple autoregressive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have $N$ observations $\\{x_t\\}$ of some time series and we want to fit an AR(1) process to this data. As it turns out, we can easily determine the parameters $w$ and $b$ simply by using trusty old least squares. Taking $x_1$ as given, then we simply solve\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_N\n",
    "\\end{bmatrix} = \\theta\\begin{bmatrix}\n",
    "x_1 \\\\ \\vdots \\\\ x_{N-1}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b \\\\ \\vdots \\\\ b\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With the formalism out of the way, let's implement this with JAX. We'll do it \"ML style\" and fit the model by optimizing a loss function rather than using any closed form formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from collections import namedtuple\n",
    "randkey = random.PRNGKey(0)\n",
    "AR1Params = namedtuple('AR1Params', 'w b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the method, let's generate some AR(1) data and try to guess what the true parameters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = random.normal(randkey)\n",
    "b = jnp.array(1.0)\n",
    "randkey, _ = random.split(randkey)\n",
    "x_1 = random.randint(randkey, (), 1, 100)\n",
    "true_params = AR1Params(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def predict(params: AR1Params, x: jnp.array) -> jnp.array:\n",
    "    return params.w * x + params.b\n",
    "vpredict = jax.vmap(predict,(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x_1]\n",
    "for _ in range(1_000):\n",
    "    data.append(predict(true_params, data[-1]))\n",
    "data = jnp.array(data)\n",
    "test_data = data[-100:]\n",
    "train_data = data[:-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit the model by optimizing the mean-squared-error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss(params: AR1Params, xs: jnp.array):\n",
    "    return jnp.square(xs[1:] - vpredict(params, xs)[:-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = AR1Params(jnp.array(0.1), jnp.array(0.1))\n",
    "g = jax.grad(loss)\n",
    "G = g(params, train_data)\n",
    "eta = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.4\n",
      "w is now 0.1\tb is now 0.1\n",
      "loss 0.12\n",
      "w is now -0.17454\tb is now 0.63699\n",
      "loss 0.019\n",
      "w is now -0.19348\tb is now 0.85669\n",
      "loss 0.003\n",
      "w is now -0.20096\tb is now 0.94342\n",
      "loss 0.00046\n",
      "w is now -0.20392\tb is now 0.97767\n",
      "loss 7.2e-05\n",
      "w is now -0.20508\tb is now 0.99118\n",
      "loss 1.1e-05\n",
      "w is now -0.20554\tb is now 0.99652\n",
      "loss 1.8e-06\n",
      "w is now -0.20572\tb is now 0.99863\n"
     ]
    }
   ],
   "source": [
    "loss_grad_fn = jax.grad(loss)\n",
    "for i in range(10_000):\n",
    "    if i % 50 == 0:\n",
    "        ell = loss(params, train_data)\n",
    "        print(f\"loss {ell:.2}\")\n",
    "        print(f\"w is now {params.w:.5}\\tb is now {params.b:.5}\")\n",
    "    G = loss_grad_fn(params, train_data)\n",
    "    old_params = params\n",
    "    params = AR1Params(params.w - eta * G.w, params.b - eta * G.b)\n",
    "    if abs(old_params.w - params.w) < 1e-6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true w: -0.20584\n",
      "computed w: -0.20579\n",
      "true b: 1.0\n",
      "computed b: 0.99938\n"
     ]
    }
   ],
   "source": [
    "print(f\"true w: {true_params.w:.5}\\ncomputed w: {params.w:.5}\\ntrue b: {true_params.b:.5}\\ncomputed b: {params.b:.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're in the ballpark! As a sanity check, let's use the `statsmodels` library to fit a first-order autoregressive model on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            AutoReg Model Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  901\n",
      "Model:                     AutoReg(1)   Log Likelihood               15292.762\n",
      "Method:               Conditional MLE   S.D. of innovations              0.000\n",
      "Date:                Sat, 12 Aug 2023   AIC                         -30579.523\n",
      "Time:                        13:32:54   BIC                         -30565.116\n",
      "Sample:                             1   HQIC                        -30574.020\n",
      "                                  901                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.0000    3.5e-10   2.86e+09      0.000       1.000       1.000\n",
      "y.L1          -0.2058   1.03e-10     -2e+09      0.000      -0.206      -0.206\n",
      "                                    Roots                                    \n",
      "=============================================================================\n",
      "                  Real          Imaginary           Modulus         Frequency\n",
      "-----------------------------------------------------------------------------\n",
      "AR.1           -4.8581           +0.0000j            4.8581            0.5000\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "mod = AutoReg(list(train_data), 1)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `y.L1` field shows the same result, as hoped."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
